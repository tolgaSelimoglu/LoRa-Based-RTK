Mean Squared Error (MSE), bir makine öğrenimi modelinin tahminlerinin doğruluğunu ölçmek için kullanılan bir hata metriğidir. Özellikle regresyon problemlerinde yaygın olarak kullanılır. MSE, tahmin edilen değerler ile gerçek değerler arasındaki farkların karesini alıp bunların ortalamasını hesaplar.

RMSE = sqrt(mean((simulatedData - experimentalData).^2));

Hata Karelerinin Ortalaması:
MSE, her bir tahminin gerçek değere ne kadar yakın olduğunu ölçer. Tahmin ile gerçek değer arasındaki farkın karesini aldığı için, büyük hatalara daha fazla ağırlık verir.

Küçük bir MSE değeri, modelin doğru tahminler yaptığını gösterir.
Büyük bir MSE değeri, modelin tahminlerinin gerçek değerlerden sapmalarının büyük olduğunu gösterir.

-----------------------------------------------------------

İzlenmesi Gereken Adımlar:
Veri Ön İşleme:

Sinyal gürültüsünü azalt (örneğin: filtreleme).
Verileri normalize et (0-1 aralığına ölçekleme).
Gerekirse sinyal özelliklerini çıkar (örneğin: RMS, genlik, frekans bileşenleri).
Model Eğitimi:

Kayıp fonksiyonu olarak Mean Squared Error (MSE) kullan.
Optimizasyon algoritması olarak Adam iyi bir seçimdir.
Değerlendirme:

Modeli test verisi üzerinde doğruluk ve genel performans açısından değerlendir.
Özellikle R² (determinasyon katsayısı) metriği regresyon için faydalıdır.

------------------------------------------------------------

1. Yoğun (Dense) Sinir Ağları (Feedforward Neural Networks)
Eğer sinyal verilerin statik ve belirli bir boyuta sahip özelliklerden (feature) oluşuyorsa, Dense Neural Networks iyi bir başlangıçtır.

Kullanım Durumu: Sinyal, bir dizi özellik (örneğin: frekans, genlik, güç) olarak temsil ediliyorsa.
Yapı:
Birkaç tam bağlantılı (fully connected) katman.
Aktivasyon fonksiyonu olarak ReLU veya sigmoid kullanımı.
Çıkış katmanında bir adet nöron (doğrusal aktivasyon, çünkü regresyon yapıyorsun).

--------------------------------------------------------------

Understanding the Adam Optimization Algorithm:

The Adam Algorithm Formulas
The Adam algorithm computes adaptive learning rates for each parameter using the first and second moments of the gradients. Let’s break down the formulas involved in the Adam algorithm:
Initialize the model parameters (θ), learning rate (α), and hyper-parameters (β1, β2, and ε).
Compute the gradients (g) of the loss function (L) with respect to the model parameters:
Update the first moment estimates (m):
Update the second moment estimates (v):
Correct the bias in the first (m_hat) and second (v_hat) moment estimates for the current iteration (t)
Compute the adaptive learning rates (α_t):
Update the model parameters using the adaptive learning rates:
This is a MATLAB implementation of the Adam optimization algorithm as described above. This implementation can be easily adapted for other loss functions and machine learning models.

----------------------------------------------------------------

gerekirse "extended kalman filter simulink" kullanılarak çıkan verideki sapmalar önlenebilir.